{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Created by Luis A. Sanchez-Perez (alejand@umich.edu).\n",
    "<p><span style=\"color:green\"><b>Copyright &#169;</b> Do not distribute or use without authorization from author.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Do not modify any of the cells (unless specified otherwise) provided in this notebook or you might have problems finishing the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part-i'></a>\n",
    "## Part I - Overview\n",
    "\n",
    "In this assignment you will implement, visualize and test different optimizers that represent a huge improvement over a basic gradient descend optimizer and do not need to compute/estimate the Hessian matrix. The optimizers you will compare are:\n",
    "* Basic gradient descend (Already implemented as example: `BatchGradientDescendOptimizer`)\n",
    "* Adagrad (Already implemented as example: `AdagradOptimizer`)\n",
    "* Gradient descent with Momentum\n",
    "* RMSProp\n",
    "* Adam\n",
    "\n",
    "There are more optimizers you might find out there (included upgraded versions) but in general, learning the ones abovementioned will give the tools to understand other related optimizers.\n",
    "\n",
    "We will minimize (as cost functions) some of the functions (with two independant variables `x` and `y`) from a [well known set of functions](https://en.wikipedia.org/wiki/Test_functions_for_optimization) that are useful to evaluate optimization algorithms. These functions are:\n",
    "* The Beale function\n",
    "* The Booth function\n",
    "* The Himmelblau function\n",
    "* The Goldstein-Price function\n",
    "\n",
    "All these functions are implemented in the library `cost_functions.py` that must be downloaded together with this notebook. The gradient of these functions is computed using the [autograd library](https://github.com/HIPS/autograd).\n",
    "\n",
    "You don't have to worry about implementing the training loop (implemented in the `optimizers_starter.py` library) or the visualization (implemented in the `training_animation.py` library). Your only task will be to correctly implement each of the optimizers following the optimizers template.\n",
    "\n",
    "This notebook consists of the following parts:\n",
    "* <p><a href=\"#part-i\")>Part I - Overview:</a> A brief description of the assignment.</p>\n",
    "* <p><a href=\"#part-ii\")>Part II - Implementing the optimizers:</a> You will code only in this part.</p>\n",
    "* <p><a href=\"#part-iii\")>Part III - Training implementation:</a> Using the optimizers to minimize the functions.</p>\n",
    "* <p><a href=\"#part-iv\")>Part IV - Testing and visualizing trainings:</a> A nice visualization of the trainings for different cost functions and optimizers.</p>\n",
    "\n",
    "<p><span style=\"color:red\"><b>Note: </b> I recommend you run the entire notebook before coding anything, paying particular attention to all cells outputs in <a href=\"#part-iv\")>Part IV</a></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing autograd\n",
    "If you don't have the autograd library already installed you must run the cell below in order to properly finish this assigment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "All the libraries specified below are needed to finish this assigment. You must download and place in the same folder this notebook and the following files:\n",
    "* `cost_functions.py`\n",
    "* `training_animation.py`\n",
    "* `optimizers_starter.py`\n",
    "* `optimizers_unit_test.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "import numpy as np\n",
    "from cost_functions import graph_function\n",
    "from cost_functions import beale, gradient_beale\n",
    "from cost_functions import booth, gradient_booth\n",
    "from cost_functions import himmelblau, gradient_himmelblau\n",
    "from cost_functions import goldstein, gradient_goldstein\n",
    "from cost_functions import approximate_gradient\n",
    "from training_animation import TrainingAnimation\n",
    "from optimizers_unit_tests import *\n",
    "try:\n",
    "    optimizers = __import__('optimizers_solution')\n",
    "except ImportError:\n",
    "    optimizers = __import__('optimizers_starter')\n",
    "BatchGradientDescendOptimizer = optimizers.BatchGradientDescendOptimizer\n",
    "AdagradOptimizer = optimizers.AdagradOptimizer\n",
    "MomentumOptimizer = optimizers.MomentumOptimizer\n",
    "RMSPropOptimizer = optimizers.RMSPropOptimizer\n",
    "AdamOptimizer = optimizers.AdamOptimizer\n",
    "train = optimizers.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple but beatiful test!\n",
    "In order to verify that `autograd` and our functions from `cost_functions.py` are working properly we are going to compute the gradient of the `himmelblau` function in three different ways (including `autograd`) at the point (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "x = 1.\n",
    "y = 2.\n",
    "value = gradient_himmelblau([x, y])\n",
    "print('Gradient using autograd: ', value)\n",
    "value = [\n",
    "    4*x*(x**2 + y - 11) + 2*(x + y**2 - 7),\n",
    "    2*(x**2 + y - 11) + 4*y*(x + y**2 - 7)\n",
    "]\n",
    "print('Gradient analytically computed: ', value)\n",
    "delta = 1e-5\n",
    "value = [\n",
    "    (himmelblau([x + delta, y]) - himmelblau([x - delta, y])) / (2*delta),\n",
    "    (himmelblau([x, y + delta]) - himmelblau([x, y - delta])) / (2*delta)\n",
    "]        \n",
    "print('Gradient using finite differences: ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part-ii'></a>\n",
    "## Part II - Implementing the optimizers\n",
    "In the file `optimizers_starter.py` you will find definitions for the following classes:\n",
    "* `Param`: A class to hold parameters.\n",
    "* `Optimizer`: An abstract class (base class) for any optimizer implemented later.\n",
    "* `BatchGradientDescendOptimizer`: An optimizer class implementing the update rule used in Batch Gradient Descend.\n",
    "* `AdagradOptimizer`: An optimizer class updating parameters using Adagrad (adaptative learning rate).\n",
    "\n",
    "Each `Optimizer` has basically three methods:\n",
    "* `__init__ (self, **opts)`: Method called when the object is created and receives a list of keywords parameters in `**opts`.\n",
    "* `update(self, params, gradients)`: Method called to update parameters passed in the iterable `params` based on their corresponding `gradients`. Notice `len(params) == len(gradients)`, so there is an one-to-one correspondance between their entries.\n",
    "* `reset(self, params)`: Method called to reset any optimizer configuration for the parameters passed in the iterable `params`. This function is called before starting training for all trainable parameters.\n",
    "\n",
    "You will have to complete the following implementatons: `MomentumOptimizer`, `RMSPropOptimizer` and `AdamOptimizer`. In order to do so you must complete methods: `__init__`, `reset` and `update`.\n",
    "\n",
    "<p><span style=\"color:red\"><b>Note: </b> Review the implementations for <b>BatchGradientDescendOptimizer</b> and <b>AdagradOptimizer</b> to better understand how to use these methods</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Implement Gradient Descend with Momentum optimizer\n",
    "In this part you are asked to implement Gradient Descend with Momentum. Your optimizer must have at least the following attributes (which are already setup):\n",
    "* `alpha`: Learning rate.\n",
    "* `beta`: Parameter to control the exponetially weighted average of past gradients (momentum).\n",
    "* `bias`: When true you optimizer must use bias correction in the computation of the momentum. correction.\n",
    "\n",
    "**Indications**\n",
    "* You are required to complete the `reset` and `update` methods (here is where the parameters should be updated).\n",
    "* You can add anything to `__init__` and create new methods if needed.\n",
    "* Your implementaton must also include bias correction.\n",
    "\n",
    "<p><span style=\"color:red\"><b>Note: </b> Go to <b>optimizers_starter.py</b> to complete your implementation. In there you should find a partial implementation.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit tests\n",
    "A good indication that your implementation is going well is not getting any `Failed` messages after running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "print('Verifying update method (without bias correction): ', end='')\n",
    "print('Passed') if momentum_update_default_unit_test() else print('Failed')\n",
    "print(\"Verifying update method (with bias correction): \", end='')\n",
    "print('Passed') if momentum_update_with_bias_unit_test() else print('Failed')\n",
    "print(\"Verifying reset method: \", end='')\n",
    "print('Passed') if momentum_reset_unit_test() else print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Implement RMSProp optimizer\n",
    "In this part you are asked to implement RMSProp. Your optimizer must have at least the following attributes (which are already setup):\n",
    "* `alpha`: Learning rate.\n",
    "* `beta`: Parameter to control the exponetially weighted average of past gradients (momentum).\n",
    "* `epsilon`: Small positive value to avoid getting zero in the denominator.\n",
    "* `bias`: When true you optimizer must use bias correction in the computation of the momentum.\n",
    "* `decay`: Indicates the learning rate decay. When zero, no decay is used.\n",
    "\n",
    "**Indications**\n",
    "* You are required to complete the `reset` and `update` methods (here is where the parameters should be updated).\n",
    "* You can add anything to `__init__` and create new methods if needed.\n",
    "* Your implementaton must also include bias correction.\n",
    "* Your implementaton must also include learning rate decay using the following formula:\n",
    "\n",
    "$$ \\alpha^{'} = (\\frac{1}{1 + \\text{decay} * \\text{epoch}}) * \\alpha$$\n",
    "\n",
    "where:\n",
    "* $\\alpha^{'}$: is the actual learning rate you should to update the parameter in question.\n",
    "* $\\alpha$ is the value passed as argument `alpha` to the optimizer.\n",
    "* $\\text{decay}$: is the value passed as argument `decay` to the optimizer.\n",
    "* $\\text{epoch}$: is the epoch number, which you could get by tracking how many times the optimizer update method has been called for the parameter in question.\n",
    "\n",
    "<p><span style=\"color:red\"><b>Note: </b> Go to <b>optimizers_starter.py</b> to complete your implementation. In there you should find a partial implementation.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit tests\n",
    "A good indication that your implementation is going well is not getting any `Failed` messages after running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "print(\"Verifying update method (without bias correction or decay): \", end='')\n",
    "print('Passed') if rmsprop_update_default_unit_test() else print('Failed')\n",
    "print('Verifying update method (with bias correction): ', end='')\n",
    "print('Passed') if rmsprop_update_with_bias_unit_test() else print('Failed')\n",
    "print('Verifying update method (with bias correction and decay): ', end='')\n",
    "print('Passed') if rmsprop_update_with_bias_decay_unit_test() else print('Failed')\n",
    "print(\"Verifying reset method: \", end='')\n",
    "print('Passed') if rmsprop_reset_unit_test() else print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Implement Adam optimizer\n",
    "In this part you are asked to implement the Adam optimizer. Your optimizer must have at least the following attributes (which are already setup):\n",
    "* `alpha`: Learning rate.\n",
    "* `beta1`: Parameter to control the exponetially weighted average of past gradients (momentum).\n",
    "* `beta2`: Parameter to control the exponetially weighted average of past gradients squared (to normalize as Adagrad).\n",
    "* `epsilon`: Small positive value to avoid getting zero in the denominator.\n",
    "* `bias`: When true you optimizer must use bias correction in the computation of the momentum.\n",
    "* `decay`: Indicates the learning rate decay. When zero, no decay is used.\n",
    "\n",
    "\n",
    "**Indications**\n",
    "* You are required to complete the `reset` and `update` methods (here is where the parameters should be updated).\n",
    "* You can add anything to `__init__` and create new methods if needed.\n",
    "* Your implementaton must also include bias correction.\n",
    "* Your implementaton must also include learning rate decay using the following formula:\n",
    "\n",
    "$$ \\alpha^{'} = (\\frac{1}{1 + \\text{decay} * \\text{epoch}}) * \\alpha$$\n",
    "\n",
    "where:\n",
    "* $\\alpha^{'}$: is the actual learning rate you should to update the parameter in question.\n",
    "* $\\alpha$ is the value passed as argument `alpha` to the optimizer.\n",
    "* $\\text{decay}$: is the value passed as argument `decay` to the optimizer.\n",
    "* $\\text{epoch}$: is the epoch number, which you could get by tracking how many times the optimizer update method has been called for the parameter in question.\n",
    "\n",
    "<p><span style=\"color:red\"><b>Note: </b> Go to <b>optimizers_starter.py</b> to complete your implementation. In there you should find a partial implementation.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit tests\n",
    "A good indication that your implementation is going well is not getting any `Failed` messages after running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "print(\"Verifying update method (withou| bias correction or decay): \", end='')\n",
    "print('Passed') if adam_update_default_unit_test() else print('Failed')\n",
    "print('Verifying update method (with bias correction): ', end='')\n",
    "print('Passed') if adam_update_with_bias_unit_test() else print('Failed')\n",
    "print('Verifying update method (with bias correction and decay): ', end='')\n",
    "print('Passed') if adam_update_with_bias_decay_unit_test() else print('Failed')\n",
    "print(\"Verifying reset method: \", end='')\n",
    "print('Passed') if adam_reset_unit_test() else print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part-iii'></a>\n",
    "## Part III - Training implementation\n",
    "\n",
    "The implementation of the training loop is given in the function `train` imported from the file `optimizers_starter.py`. Although you don't have to add or change anything in this code is good to review and see how the optimizers definitions are used. Notice:\n",
    "* A training is performed for each optimizer passed in `optimizers`.\n",
    "* The function to minimize is defined by the callable `cost_func` and its gradient is computed using `grad_func`.\n",
    "* Results for each optimizer are stored in a dictionary named `results` where each optimizer is a key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part-iv'></a>\n",
    "## Part IV - Testing and visualizing trainings \n",
    "In the cells following below we will test some optimizers configurations training four different cost functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beale cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring optimizers and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point to initialize the params\n",
    "point = [-2,3]\n",
    "# Creates different optimizers to train on\n",
    "optimizers = [\n",
    "    BatchGradientDescendOptimizer(alpha=1e-4),\n",
    "    AdagradOptimizer(alpha=1),\n",
    "    MomentumOptimizer(alpha=1e-4, bias=True),\n",
    "    RMSPropOptimizer(alpha=1, decay=0.1),\n",
    "    AdamOptimizer(alpha=1)\n",
    "]\n",
    "# Trains with the different optimizers\n",
    "log = train(point, optimizers, beale, gradient_beale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting cost function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_function(beale, xrange=[-4.5, 4.5], yrange=[-4.5, 4.5], title='Beale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing cool animation of the training!\n",
    "You can press `Esc` anytime to finish animation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingAnimation(log, beale, gradient_beale).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booth cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring optimizers and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point to initialize the params\n",
    "point = [0,-10]\n",
    "# Creates different optimizers to train on\n",
    "optimizers = [\n",
    "    BatchGradientDescendOptimizer(alpha=1e-2),\n",
    "    AdagradOptimizer(alpha=1),\n",
    "    MomentumOptimizer(alpha=1e-2, bias=True),\n",
    "    RMSPropOptimizer(alpha=1, decay=0.1),\n",
    "    AdamOptimizer(alpha=1)\n",
    "]\n",
    "log = train(point, optimizers, booth, gradient_booth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting cost function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_function(booth, xrange=[-10, 10], yrange=[-10, 10], title='Booth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing cool animation of the training!\n",
    "You can press `Esc` anytime to finish animation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingAnimation(log, booth, gradient_booth).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Himmelblau cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring optimizers and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point to initialize the params\n",
    "point = [0,4]\n",
    "# Creates different optimizers to train on\n",
    "optimizers = [\n",
    "    BatchGradientDescendOptimizer(alpha=1e-3),\n",
    "    AdagradOptimizer(alpha=1e-1),\n",
    "    MomentumOptimizer(alpha=1e-3, bias=True),\n",
    "    RMSPropOptimizer(alpha=1e-1, decay=0.1),\n",
    "    AdamOptimizer(alpha=1e-1)\n",
    "]\n",
    "log = train(point, optimizers, himmelblau, gradient_himmelblau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting cost function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_function(himmelblau, xrange=[-5, 5], yrange=[-5, 5], title='Himmelblau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing cool animation of the training!\n",
    "You can press `Esc` anytime to finish animation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingAnimation(log, himmelblau, gradient_himmelblau).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goldstein cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring optimizers and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point to initialize the params\n",
    "point = [0,0]\n",
    "# Creates different optimizers to train on\n",
    "optimizers = [\n",
    "    BatchGradientDescendOptimizer(alpha=1e-5),\n",
    "    AdagradOptimizer(alpha=1e-1),\n",
    "    MomentumOptimizer(alpha=1e-5, bias=True),\n",
    "    RMSPropOptimizer(alpha=1e-1, decay=0.1),\n",
    "    AdamOptimizer(alpha=1e-1)\n",
    "]\n",
    "log = train(point, optimizers, goldstein, gradient_goldstein)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting cost function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_function(goldstein, xrange=[-2, 2], yrange=[-2, 2], title='Goldstein')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing cool animation of the training!\n",
    "You can press `Esc` anytime to finish animation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingAnimation(log, goldstein, gradient_goldstein).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
